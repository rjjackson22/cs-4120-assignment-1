{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1\n",
    "Riley Jackson<br>\n",
    "Dr. Antonio Bolufe-Rohler<br>\n",
    "February 25th, 2020<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chosen Dataset & Rational\n",
    "The dataset examined in this assignment is the 'Forest Fire' dataset provided by\n",
    "[Cortez and Morais, (2007)](https://repositorium.sdum.uminho.pt/handle/1822/8039).\n",
    "This dataset of 517 examples poses a challenging multi-variate regression task, where 4-8 meteorological features are used to\n",
    "predict the area (in Hectare) of forest burned in a day. These features are as follows:\n",
    "0. Fine Fuel Moisture Code\n",
    "1. Duff Moisture Code\n",
    "2. Drought Code\n",
    "3. Initial Spread Index\n",
    "4. Temperature (in Celsius)\n",
    "5. Relative Humidity (in percent)\n",
    "6. Wind (in Km/h)\n",
    "7. Rainfall (in mm/m<sup>2</sup>)\n",
    "\n",
    "Regression algorithms are of particular importance in machine learning (ML) due to their adaptability. Indeed, regressors\n",
    "are capable of predicting real, continuous values, but they may also be utilized for classification with minimal\n",
    "modification. Regressors also fit more naturally into the description of ML as the study of generalized\n",
    "function predictors, as one often thinks of a mathematical function as outputting a real value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading, Sanitizing, & Visualizing the Data\n",
    "This dataset has several quirks which may influence both preprocessing and model selection. These include the\n",
    "difference in feature scales, the bias towards little to no area burned for each day, and the inclusion of some\n",
    "potentially irrelevant features. Before digging into these quirks, the data must first be loaded:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "average area burned: 12.847292069632493\n",
      "average value of feature 0: 90.6446808510636\n",
      "average value of feature 1: 110.87234042553195\n",
      "average value of feature 2: 547.9400386847191\n",
      "average value of feature 3: 9.021663442940042\n",
      "average value of feature 4: 18.88916827852998\n",
      "average value of feature 5: 44.28820116054158\n",
      "average value of feature 6: 4.017601547388782\n",
      "average value of feature 7: 0.02166344294003869\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function for convenience and to avoid global state\n",
    "def get_data(first_feature: int = 4):\n",
    "    data = np.genfromtxt('forestfires.csv', delimiter=',')  # load from .csv\n",
    "    x = data[1:, first_feature:12]  # The .csv also contains location and time data, which is irrelevant to the prediction task\n",
    "    y = data[1:, 12]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def check_stats():\n",
    "    x, y = get_data()\n",
    "    print('average area burned: {}'.format(np.mean(y)))\n",
    "    [print('average value of feature {}: {}'.format(i, u)) for i, u in enumerate(np.mean(x, axis=0))]\n",
    "\n",
    "check_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When looking at the average values of our features and our targets, the need for normalization is quite obvious.\n",
    "Feature 1 has an average value of **~110**, while feature 7 has an average of **~0.02**. To make the prediction\n",
    "task easier for our models, we can *normalize* our data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since this is a high dimensional dataset, it's quite difficult to visualize the data directly. We could attempt to\n",
    "select only 1 - 2 features for plotting against our y value, but that leaves us with several different possible\n",
    "plots. More effective is the use of dimensional reduction through **Principle Component Analysis** (PCA)\n",
    "to project our data into a 2D or 3D space:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "Total Variance ratio for PCA projection into R^1 space: 0.9626379470674843\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # Require for matplotlib to plot in 3D\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def two_d_plot(fig, i, j):\n",
    "    axis: plt.Axes = fig.add_subplot()\n",
    "    axis.scatter(i, j)\n",
    "    axis.set_xlabel('Principle Component')\n",
    "    axis.set_ylabel('Area Burned')\n",
    "\n",
    "def three_d_plot(fig, i, j, k):\n",
    "    axis: plt.Axes = fig.add_subplot(111, projection='3d')\n",
    "    axis.scatter(i, j, k,\n",
    "    )\n",
    "    axis.set_xlabel('Principle Component 1')\n",
    "    axis.set_ylabel('Principle Component 2')\n",
    "    axis.set_zlabel('Area Burned')\n",
    "\n",
    "def plot_pca():\n",
    "    x, y = get_data()\n",
    "    n_components = 1  # Change this value to 2 to view a 3D plot\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = np.expand_dims(pca.fit_transform(x), axis=-1)  # (M, N, 1)\n",
    "\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    if n_components == 1:\n",
    "        two_d_plot(fig, components, y)\n",
    "    elif n_components == 2:\n",
    "        three_d_plot(fig, components[:, 0], components[:, 1], y)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('Total Variance ratio for PCA projection into R^{} space: {}'.format(\n",
    "        n_components, np.sum(pca.explained_variance_ratio_)\n",
    "    ))\n",
    "\n",
    "plot_pca()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA is very useful for visualization, because now a single dimension represents all 8 of the original dataset\n",
    "with a variance conservation of **~96%**. From the above plot we can see that our data is quite tightly packed\n",
    "with-respect-to our y values, but there do exists some outliers. We can assume any regressor we train will\n",
    "do poorly on these outliers as it will bias towards smaller areas. These handful of outliers and the overall\n",
    "small number of examples in the dataset suggests we should attempt to use k-folds cross validation. Otherwise, we\n",
    "run the risk of obtaining a static test split which varies greatly from our train split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Selection & Evaluation\n",
    "Now that our data is fit for learning, we need to make decide which algorithms are applicable to our dataset. We can't\n",
    "be sure our dataset is either linear or non-linear, so we should begin with **linear regression** (LR). Also, since our\n",
    "targets are tightly packed, it may be beneficial to obtain compare our results to a 'mean predictor', which is a model\n",
    "which only ever predicts the mean value of y. This provides us an sort of *upper bound* to our predictions, so we\n",
    "know when a model is performing well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LR has a **convex** loss landscape, meaning we can obtain a true line of best fit. It will also\n",
    "let us know the degree of linearity in our data, and the parameters of the model can potentially give us some insight\n",
    "into which features are most important in the prediction task:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "CV TRAIN MAE for LR: 19.247269660445614\n",
      "CV TEST MAE for LR: 21.020804726378245\n",
      "Mean Predictor MAE: 18.566830958251177\n",
      "Param for feature 0: -0.029429883920658102\n",
      "Param for feature 1: 0.05999250030608885\n",
      "Param for feature 2: -0.0004943516814239611\n",
      "Param for feature 3: -1.0100044550537683\n",
      "Param for feature 4: 0.9560307654797862\n",
      "Param for feature 5: -0.29647143355574784\n",
      "Param for feature 6: 2.0512882238202885\n",
      "Param for feature 7: -2.6494255844916923\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from functools import partial\n",
    "\n",
    "cross_validate = partial(\n",
    "    cross_validate,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "def mean_predictor_mae():\n",
    "    _, y = get_data()\n",
    "    y_pred = np.empty_like(y)\n",
    "    y_pred[:] = np.mean(y)\n",
    "    return mean_absolute_error(y, y_pred)\n",
    "\n",
    "def mean_results(results):\n",
    "    return np.mean(\n",
    "        np.absolute(\n",
    "            results\n",
    "        )\n",
    "    )\n",
    "\n",
    "def print_regressor_results(results, name='model'):\n",
    "    print('CV TRAIN MAE for {}: {}'.format(name, mean_results(results['train_score'])))\n",
    "    print('CV TEST MAE for {}: {}'.format(name, mean_results(results['test_score'])))\n",
    "\n",
    "def evaluate_linear_regression():\n",
    "    x, y = get_data()\n",
    "    results = cross_validate(\n",
    "        LinearRegression(normalize=True),\n",
    "        x,\n",
    "        y\n",
    "    )\n",
    "    print_regressor_results(results, 'LR')\n",
    "    print('Mean Predictor MAE: {}'.format(mean_predictor_mae()))\n",
    "    [print('Param for feature {}: {}'.format(i, p)) for i, p in enumerate(results['estimator'][0].coef_)]\n",
    "\n",
    "evaluate_linear_regression()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see from the above results that LR performs worse than a simple mean predictor, and we observe a slight\n",
    "difference between train and test results. This is likely due to the test set not being reflective of the train\n",
    "set, but it may also indicate **overfitting**. It's always worth adding some **regularization** to combat overfitting,\n",
    "so we can make note of that when deciding on our next model. Also, it may be likely that the dataset is non-linear,\n",
    "so we should attempt to fit a **non-linear** model. Lastly, as suggested by the authors of this dataset, the parameters\n",
    "for features 0 - 2 are quite **small**, suggesting they are less important in the prediction task.\n",
    "It may be beneficial to only make predictions using **features 3 - 7** moving forward."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll begin by testing the **feature selection** of features 3 - 7 only:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "CV TRAIN MAE for LR: 19.06048325052673\n",
      "CV TEST MAE for LR: 20.395378491493904\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def evaluate_linear_regression_with_feature_selection():\n",
    "    x, y = get_data(first_feature=8)  # Select elements [8 - 12) of .csv rows\n",
    "    results = cross_validate(\n",
    "        LinearRegression(normalize=True),\n",
    "        x,\n",
    "        y\n",
    "    )\n",
    "    print_regressor_results(results, 'LR')\n",
    "\n",
    "evaluate_linear_regression_with_feature_selection()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe slightly better results, although very minor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we didn't observe a significant difference in the test/train errors, we'll next test out a non-linear model.\n",
    "The most simple example of this is a polynomial regressor:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "CV TRAIN MAE for Poly LR with degree 2: 18.753777239557742\n",
      "CV TEST MAE for Poly LR with degree 2: 21.564333481864704\n",
      "CV TRAIN MAE for Poly LR with degree 4: 19.093717511253264\n",
      "CV TEST MAE for Poly LR with degree 4: 24.904667911903637\n",
      "CV TRAIN MAE for Poly LR with degree 8: 22.542648791757408\n",
      "CV TEST MAE for Poly LR with degree 8: 110114.34251892597\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def evaluate_polynomial_regressor():\n",
    "    x, y = get_data(8)\n",
    "    for degree in [2, 4, 8]:\n",
    "        model = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=degree)),\n",
    "            ('regressor', LinearRegression(False, normalize=True))\n",
    "        ])\n",
    "        results = cross_validate(\n",
    "            model,\n",
    "            x,\n",
    "            y\n",
    "        )\n",
    "        print_regressor_results(results, 'Poly LR with degree {}'.format(degree))\n",
    "\n",
    "evaluate_polynomial_regressor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we increase the degree of the polynomial, we observe *severe* overfitting. We also notice a slight decrease in\n",
    "performance on the training set. A polynomial regressor is likely not optimal for this prediction task, something\n",
    "we could have anticipated. Non-linear regressors are very susceptible for overfitting at high polynomial degrees. Also,\n",
    "their cost landscapes are *non-convex*, meaning they must be minimized using iterative methods, e.g. by\n",
    "gradient descent. Non-convex functions also denote the presence of **local minima**, so polynomial regressors\n",
    "can be somewhat unstable. There do exist other ML algorithms for learning on non-linear data more fit for this\n",
    "task that we shall discuss next."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the LR and polynomial regressors we previously studied we observed two major requirements from our model:\n",
    "1. We need to utilize some amount of **regularization** to ensure we don't overfit our data\n",
    "2. We should try another form of **non-linear** fitting\n",
    "\n",
    "A class of models that fit these two requirements are the **kernel ridge regressors** (KRR). KRRs utilize the\n",
    "kernel trick of transforming our data into a new, potentially linear space, and they also employ l2-regularization\n",
    "for combatting overfitting. Unlike LR, they also have an ample amount of hyperparameters that can be tuned\n",
    "to squeeze out as much performance as possible. `sklearn` has an easily tunable KRR class we can attempt\n",
    "to fit to our dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Best model params: {'alpha': 100, 'gamma': 100, 'kernel': 'rbf'}\n",
      "CV TRAIN MAE for Best KRR: 12.720311064523774\n",
      "CV TEST MAE for Best KRR: 12.893793113959635\n",
      "Worst model params: {'alpha': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "CV TRAIN MAE for Worst KRR: 12.771171662255437\n",
      "CV TEST MAE for Worst KRR: 27.4289639492486\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate_krr():\n",
    "    x, y = get_data(first_feature=8)  # Select elements [8 - 12) of .csv rows\n",
    "    search_space = {  # 16 total models\n",
    "        'alpha': [0.1, 1, 10, 100],  # alpha = lambda / 2, where lambda is the level of regularization used\n",
    "        'kernel': ['rbf'],  # linear doesn't transform the space, rbf uses a Gaussian kernel\n",
    "        'gamma': [0.1, 1, 10, 100]  # Width of the Gaussian, denotes the amount of influence each example has in the transformation\n",
    "    }\n",
    "    model = GridSearchCV(\n",
    "        KernelRidge(),\n",
    "        search_space,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    model = model.fit(x, y)\n",
    "    print('Best model params: {}'.format(model.best_params_))\n",
    "    results = cross_validate(model.best_estimator_, x, y)\n",
    "    print_regressor_results(results, 'Best KRR')\n",
    "\n",
    "    worst_params = model.cv_results_['params'][0]\n",
    "    worst_results = cross_validate(KernelRidge(**worst_params), x, y)\n",
    "    print('Worst model params: {}'.format(worst_params))\n",
    "    print_regressor_results(worst_results, 'Worst KRR')\n",
    "\n",
    "\n",
    "evaluate_krr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "KRR performs very well on this dataset, obtaining the lowest MAE values tested yet. This error, **12.89 Ha**,\n",
    "is very close to the  **12.72 Ha** value obtained by the model proposed by the original authors, Cortez and Morais. They utilized\n",
    "**support vector regression** (SVR), a modified SVM designed for regression tasks.\n",
    "SVR and KRR are almost identical, as they both utilized l2-regularization and the kernel trick. Their major difference\n",
    "is their cost functions; KRR uses mean squared error, while SVR uses the hinge loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVR and KRR often converge to similar results, so it is possible that the **0.17 Ha** difference in performance\n",
    "is due to the relatively small size of our KRR search space. This small search space was chosen to\n",
    "conserve computational time and resources while still demonstrating the possible spread of model performance.\n",
    "As we can see, our worst performing KRR model still performs quite well on the *train set*, but we can see that\n",
    "it has overfit our data quite significantly. Since $\\alpha \\propto \\lambda$, where lambda controls the level of\n",
    "regularization utilized when training the model, the performance of the worst model makes sense when we view its\n",
    "parameters, as $\\alpha = 0.1$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we should attempt to understand the bias of our best KRR model. From our PCA plot, we suspected that our models\n",
    "will perform poorly on examples with a large area burned. We can confirm this by sorting our model's predictions\n",
    "by the largest errors view the examples that produced these errors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[746.28 174.63 43.32 40.54 31.72 30.32 30.18 28.19 27.35 24.23 16.33 15.34\n",
      " 14.29 13.70 11.32 11.24 11.22 10.02 9.96 9.41 8.68 8.24 8.00 7.80 7.36\n",
      " 6.96 6.84 6.83 6.61 6.36 6.30 6.10 5.80 5.55 3.93 3.19 3.05 2.87 2.74\n",
      " 2.69 2.57 2.44 2.29 2.18 2.14 2.01 2.00 1.94 1.63 1.63 1.46 1.36 1.26\n",
      " 1.07 0.95 1.01 0.72 0.71 0.54 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def check_worst_examples():\n",
    "    x, y = get_data(8)\n",
    "    model = KernelRidge(alpha=100, kernel='rbf', gamma=100)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    worst_examples = np.argsort(np.absolute(y_test - y_pred))[::-1]  # Sort in descending order\n",
    "    np.set_printoptions(formatter={'float': lambda l: \"{0:0.2f}\".format(l)})  # Ensure not to print in scientific not.\n",
    "    print(y_test[worst_examples])\n",
    "\n",
    "check_worst_examples()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The largest values **always** appear at the top of our array sorted by worst performance, even if the train set\n",
    "contains a large amount of high valued y's. This confirms our suspicions that our best model is biased towards\n",
    "predicting smaller burn areas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the three model types tests, we can suggest that the Forest Fire dataset is a non-trivial prediction task.\n",
    "The data appears to be non-linear, as evidented by the increase in performance when applying the kernel trick\n",
    "with KRR. Also, a number of features seem to be irrelevant to the prediction task. These irrelevant features\n",
    "are the first four features of the dataset:\n",
    "\n",
    "0. Fine Fuel Moisture Code\n",
    "1. Duff Moisture Code\n",
    "2. Drought Code\n",
    "3. Initial Spread Index\n",
    "\n",
    "While the relevant features are:\n",
    "\n",
    "4. Temperature (in Celsius)\n",
    "5. Relative Humidity (in percent)\n",
    "6. Wind (in Km/h)\n",
    "7. Rainfall (in mm/m<sup>2</sup>)\n",
    "\n",
    "This suggests that with only meteorological data one can predict the burned area of a forest within a reasonable error\n",
    "margin of $\\pm13$ Hectare. However, our models are not without bias. Since the Forest Fire dataset is skewed towards\n",
    "smaller burn areas, this is reflected in our model. A model is only as good as the data it is trained upon, so it\n",
    "is likely this bias will be intrinsic to all models trained on this dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}